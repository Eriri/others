\documentclass{ieeeaccess}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\history{Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.}
\doi{10.1109/ACCESS.2017.DOI}

\title{Belief rule base inference method based on gradient descent with momentum}
\author{\uppercase{First A. Author}\authorrefmark{1}, \IEEEmembership{Fellow, IEEE},
    \uppercase{Second B. Author\authorrefmark{2}, and Third C. Author,
        Jr}.\authorrefmark{3},
    \IEEEmembership{Member, IEEE}}
\address[1]{National Institute of Standards and
    Technology, Boulder, CO 80305 USA (e-mail: author@boulder.nist.gov)}
\address[2]{Department of Physics, Colorado State University, Fort Collins,
    CO 80523 USA (e-mail: author@lamar.colostate.edu)}
\address[3]{Electrical Engineering Department, University of Colorado, Boulder, CO
    80309 USA}
\tfootnote{This paragraph of the first footnote will contain support
    information, including sponsor and financial support acknowledgment. For
    example, ``This work was supported in part by the U.S. Department of
    Commerce under Grant BS123456.''}

\markboth
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}

\corresp{Corresponding author: First A. Author (e-mail: author@ boulder.nist.gov).}

\begin{abstract}

    The belief-rule-base(BRB) inference methodology using evidential reasonging(ER)
    approach is widely used in different fields, such as  fault diagnosis, system
    identification and decision analysis.
    In this paper, we propose a new belief rule structure and its training method,
    aiming to solve zero activation during the inference process and improve inference accuray.
    We first used the Gaussian function to calculate the similarity of each attribute instead of the original method.
    Then we introduce corresponding attribute weight for each rule and cancel the rule weight parameter at the same time.
    Finally, we use the momentum optimized gradient descent with method for parameters training based on the new rule structure.
    Experiments on several public classification datasets are conducted to validate the proposed approach compared with some recent existing works.
    The experimental results show that the proposed approach have a better performence in accuray and time consumption.

\end{abstract}

\begin{keywords}
    belief rule base, structure optimization, stochastic gradient descent, momentum optimization.
\end{keywords}

\titlepgskip=-15pt

\maketitle

\section{Introduction}
\label{sec:introduction}

The belief rule-based inference methodology using evidential reasonging approach(RIMER) proposed by Yang\cite{a1}
baesd on traditional IF-THEN rules\cite{a2}, Dempster-Shafer theory of evidence\cite{a3,a4}, decision theory\cite{a5}
and fuzzy set theory\cite{a6}. By introducing a belief distribution structure in the rules, this methodology can
effectively handle incomplete and uncertain information involved in the datasets and widely used in various problem in
different fields such as oil pipeline leak detection\cite{a7}, military capability estimation\cite{a8}, consumer behavior
prediction\cite{a9} and so on.

In the inference process of the BRB system, the attribute weight, rule weight, belief distribution and other parameters
directly affect the final accuray. Yang\cite{a10} proposed optimization models for training BRB system using fmincon solver in
Matlab, Chang\cite{a11,a12} proposed an algorithm for training parameters in BRB system based on gradient and dichotomy methods,
Wu\cite{a13} used the accelerating of gradient algorithm to improve the convergence accuray and convergence speed. There are also
a series of intelligent algorithms such as the particle swarm algorithm proposed by Su\cite{a14} and the differential evolution
algorithm proposed by Wang\cite{a15} have excellent training effects on the BRB system. Liu\cite{a16} introduces the belief distribution
structure into the antecedent attributes and uses training data to build an extended belief rule base(EBRB) system, which simplifies the construction of
the rule base and improves the inference speed.

At present, the parameter optimization model of the BRB system is mostly based on various intelligent algorithms. Their process is
complicated and there are many intermediate training parameters. When the traditional gradient method is used to train the parameters
of the BRB system, the step size is restricted by a variety of constraints, and other methods are needed to find the optimal step size.
The EBRB system does not introduce a parameter training process, which makes the system have higher requirements for the representativeness
of the training data selected to build the rule base. In the case of a large number of rules, it is necessary to perform rule reduction or
use the data structure to optimize the storage and activation process of the rules. Because the traditional BRB system includes the rule attribute
reference level setting, its potential zero activation problem may cause the inference system to fail.

In response to the above problems, we have proposed a series of optimization modifications to the system structure and reasoning process, including:

\quad 1) We propose a new antecedent structure that does not need to set the attribute reference level, and proposed a Gaussian function-based
rule weight activation method for the new rule antecedent structure, which can effectively avoid the zero activation problem and has the feature of
generating rules from the training data like EBRB.

\quad 2) We change the method of setting the weight of the global same antecedent attribute in the
traditional BRB system, and set the corresponding rule attribute weight parameter for each rule, so that each rule has a finer activation granularity.
On this basis, the rule weight and its related normalization process are cancelled, which simplifies the evidential reasoning process.

\quad 3) We further introduce the normalized exponential function to preprocess the restricted parameters to avoid the problem of
parameter failure during the training process.

The remainder of this paper is organized as follows: Section II introduces the traditional BRB system and our further improvements for common problems in the system.
In Section III, we give the preprocessing method of the training model and prove that the gradient descent method can be effectively applied to the newly proposed BRB system.
In Section IV, we compare the effects of different gradient descent parameters on training speed and inference accuracy. Experiments on a series of public data sets prove that
the newly proposed BRB model and its training method have excellent performance. Section V concludes this paper.


\section{BRB system with new attribute structure and rule activation weight calculation method}
The BRB system proposed by Yang mainly refers to the rule activation and evidence reasoning method on the belief rule base. This section will briefly introduce
the related concepts of the BRB system and propose solutions to the common defects of the traditional BRB system.
\subsection{representation of belief rule base}
On the basis of the traditional production rules, Yang\cite{a1} proposed the expression form of the belief rules by introducing the belief distribution structure,
the rule antecedent attribute parameter and the rule weight parameter. The specific expression is as follows:
$$R_k:if\{X_1isA_1^k \wedge \cdots \wedge X_{T_k}isA_{T_k}^k\}$$
$$then\{(D_1,\beta_1^k),\cdots,(D_N,\beta_N^k)\},\sum_{i=1}^N\beta_i^k\leq1$$
The equal sign is obtained when the rule information is complete. Each rule has its rule weight $\theta_k$, antecedent attribute weight $\delta_{1},\delta_{2},\cdots,\delta_{T_k}$.
$A_i^k$ represents the candidate reference value selected by the rule on the $i$-th attribute and $\beta_i^k$ represents the belief degree of the rule in the $i$-th result attribute.
On this basis, the extended belief rule base system introduces a belief distribution structure to the antecedent attributes, and its rule form is expressed as follows:
$$R_k:if\{[(A_{11}^k,\alpha_{11}^k),\cdots,(A_{1J_1}^k,\alpha_{1J_1}^k)] \wedge $$
$$\cdots \wedge [(A_{T_k1}^k,\alpha_{T_k1}^k), \cdots,(A_{T_kJ_{T_k}}^k,\alpha_{T_kJ_{T_k}}^k)]\}$$
$$then\{(D_1,\beta_1^k),\cdots,(D_N,\beta_N^k)\},\sum_{i=1}^N\beta_i^k\leq1$$
The extended belief rule base constructed using the original data transforms the input data into the rule antecedent attributes in the form of belief distribution. For the input data:
$$X^k=(x_1^k,\cdots,x_T^k)$$
Convert the $i$th attribute parameter to construct the $i$th antecedent attribute of the corresponding rule with a belief distribution form:
$$\alpha_{ij}^k=\frac{\gamma_{i(j+1)-x_i^k}}{\gamma_{i(j+1)-\gamma_{ij}}},\gamma_{ij}\leq x_i^k\leq \gamma_{i(j+1)}$$
$$\alpha_{i(j+1)}^k=1-\alpha_{ij}^k,\gamma_{ij}\leq x_i^k\leq \gamma_{i(j+1)}$$
$$\alpha_{it}^k=0,t=1,\cdots,(j-1),(j+2),\cdots,J_i$$
According to the same conversion method, the values of original data on other attributes can be converted into the corresponding berlief distribution form.
We can also obtain the belief distribution form of the rule result attribute according to this method.

\subsection{evidence reasoning approach based on belief rule base}
The calculation and synthesis of activation weights for each rule in the rule base is the core part of the inference system of the belief rule base.
The whole process mainly includes two steps: calculate the activation weight, synthesize the rules according to the activation weight.
The calculation of the activation weight of each rule in the belief rule base can be regarded as calculating the belief distribution similarity on each attribute and combining their results.
Euclidean distance is used to calculate the individual matching degree of the $i$-th attribute. After converting the input data to have the same belief distribution form as the corresponding attribute,
the individual matching degree of the attribute is calculated as:
$$S_i^k=1-d_i^k=1-\sqrt{\frac{\sum_{j=1}^{J_i}(\alpha_{i,j}-\alpha_{i,j}^k)^2}{2}}$$
After the individual matching degree of each attribute is calculated, the individual matching degrees of all attributes are aggregated. The aggregation function in the form of conjunctive rules is:
$$\alpha_k=\prod_{i=1}^{T_k}(S_i^k)^{\overline{\delta_i}},\overline{\delta_i}=\frac{\delta_i}{\max_{j=1,\cdots,T_k}\delta_j}$$
The activation weight of this rule is calculated by the following formula:
$$w_k=\frac{\theta_k\alpha_k}{\sum_{l=1}^L\theta_l\alpha_l}$$
Rule weight normalization operation makes all weights satisfy $0\leq w_k\leq 1,\sum w_k=1$.

After the rule weight calculation is completed, all the rules are synthesized and the inference result is obtained.
First, the belief distribution of the rule is transformed into the corresponding probability quality information:
$$m_{j,k}=w_k\beta_j^k,j=1,\cdots,N$$
$$m_{D,k}=1-\sum_{j=1}^Nm_{j,k}=1-w_k\sum_{j=1}^{N}\beta_j^k$$
$$\overline{m}_{D,k}=1-w_k,\quad\widetilde{m}_{D,k}=w_k(1-\sum_{j=1}^N\beta_j^k)$$
$m_{j,k}$ represents the credibility of the $k$ rule on the $j$ result attribute,
$\overline{m}_{D,k}$ represents the credibility that the $k$-th rule is not assigned to any result attribute,
$\widetilde{m}_{D,k}$  represents the credibility of the missing reference attribute of the result of the $k$-th rule,
The total uncertainty credibility is given by $m_{D,k}=\overline{m}_{D,k}+\widetilde{m}_{D,k}$.
Synthesize according to the credibility information of each rule and get the final belief result of the $j$-th result attribute:
$$m_j=k[\prod_{i=1}^L(m_{j,i}+m_{D,i})-\prod_{i=1}^Lm_{D,i}]$$
$$\overline{m}_D=n[\prod_{i=1}^L\overline{m}_{D,i}],\quad\widetilde{m}_D=k[\prod_{i=1}^Lm_{D,i}-\prod_{i=1}^L\overline{m}_{H,i}]$$
$$k=[\sum_{j=1}^N\prod_{i=1}^L(m_{j,i}+m_{D,i})-(N-1)\prod_{i=1}^Lm_{D,i}]^{-1}$$
$$\beta_j=\frac{m_j}{1-\overline{m}_D},\quad\beta_D=\frac{\widetilde{m}_D}{1-\overline{m}_D}$$

\subsection{new attribute structure and rule activation weight calculation method}
The process of generating rules in the rule base requires artificial setting of candidate reference values for the antecedent attribute information,
and when calculating the rule activation weight, if the input attribute information is not in the adjacent area of the rule attribute reference value, the rule cannot be activated.
If all the rules in the library are not activated, the reasoning system will fail. In order to solve the above problems, we proposes an improved form of belief rules and corresponding activation weight calculation method as follows:
$$R_k:if(x_1^k , \cdots , x_{T_k}^k)$$
$$then\{(D_1,\beta_1^k),\cdots,(D_N,\beta_N^k)\},\sum_{i=1}^N\beta_i^k\leq1$$
The simplified belief rule structure can directly use the training data to generate the rule antecedent attribute information without manually setting the candidate reference values of the antecedent attributes.

Using antecedent attribute belief distribution similarity as the activation weight calculation method is no longer applicable to the simplified confidence rule form. In order to perform effective weight activation,
we uses Gaussian function to calculate the individual matching degree for activation weight calculation.
The degree of individual matching of input data $X=(x_1,\cdots,x_(T_k))$ and rule $R_k:if(x_1^k , \cdots , x_{T_k}^k)then\{(D_1,\beta_1^k),\cdots,(D_N,\beta_N^k)\}$ on $i$-th attribute is calculated using the Gaussian function as:
$$S_i^k=e^{-[a_i^k\times(x_i-x_i^k)]^2}$$
The parameter $a_i^k$ represents the sensitivity of the $i$-th attribute to the distance at the position $x_i^k$. When the distance between the rule antecedent attribute and the input data remains unchanged,
the value of parameter $a$ inversely proportional to the matching degree.
The activation weight of a single rule under conjunctive conditions is calculated by the following formula:
$$w_k=\prod_{i=1}^{T_k}S_i^k=e^{-\sum_{i=1}^{T_k}[a_i^k(x_i-x_i^k)]^2}$$


Assuming a rule with two attributes x and y located at the origin, the traditional method and Gaussian function method are used to calculate the activation weights.
Set the reference candidate values on the x-attributes and y-attributes to be both $[-4,-3,-2,-1,0,1,2,3,4]$,
and set the distance-sensitive parameter $a$ of each attribute at the origin is $0.5$.
For the convenience of calculation, we omitted the setting of rule weight.
The two activation weight distributions shown in Figure \ref{fig1} and Figure \ref{fig2} can be obtained.
\Figure[!t](topskip=0pt, botskip=0pt, midskip=0pt)[width=3.3 in]{Figure_1.png}
{activation weight calculated by traditional methods\label{fig1}}
\Figure[!t](topskip=0pt, botskip=0pt, midskip=0pt)[width=3.3 in]{Figure_2.png}
{activation weight calculated by Gaussian function methods\label{fig2}}

According to Figure 1, we can know that the activation weight calculated by the input data that is not in the adjacent area of the candidate value selected by the rule is zero.
If the activation weight of all rules is zero, the inference cannot be performed.
However, the input data in Figure 2 smoothly drops close to zero according to the distance from the rule and will not take a value of zero.
This eliminates the impact of rule zero activation on system inference performance.

Another benefit brought by the new rule antecedent attribute structure and rule activation weight calculation method is that
there is no need to adjust the activation weight of the rule by the attribute weight and rule weight.
By adjusting the distance-sensitive parameters on each attribute of each rule, a good activation effect and activation granularity can be obtained.
At the same time, due to the characteristics of the Gaussian function, the activation weight of each rule belongs to $(0, 1]$ without unnecessary normalization operations.
This greatly simplifies the redundant weight adjustment and calculation in the inference process


\section{Momentum optimized Gradient descent training parameter}
When the traditional gradient method is applied to the parameter training process of the inference system of the belief rule base,
it is difficult to construct the partial derivative formula of the rule attribute and the training step is restricted by the parameter constraints.
The above-mentioned belief rule base reasoning system with improved structure and activation method avoids the difficulty of obtaining partial derivatives in traditional belief rule systems.

In this section we will:

\quad 1) Calculate the partial derivatives of the parameters of each part of the new brb system

\quad 2) Prove that the BRB inference system is differentiate

\quad 3) Introduce exponential normalization function for preprocessing to avoid specific constraints during parameter training

\quad 4) Introduce the stochastic gradient descent method using momentum optimization


\subsection{Partial derivative of the parameters of the brb system}
The reasoning process of the improved BRB system is shown in Figure \ref{fig3}.
According to the data flow path of the inference system, we can use the compound function chain derivation rule to obtain the partial derivative of the output to different parameters of the inference system.
\Figure[!t](topskip=0pt, botskip=0pt, midskip=0pt)[width=3.3 in]{process.png}
{Inference system operation process\label{fig3}}

Since the model construction and experiment in this paper are carried out with complete data, the evidence reasoning process can be simplified.
The belief distribution of the rule result attribute does not include uncertain information, that is, for any k-th rule:
$$\sum_{i=1}^N\beta_{ik}=1(k=1,\cdots,L)$$
$$m_{D,k}=\overline{m}_{D,k},\widetilde{m}_{D,k}=0$$
In the case of completeness, the $j$-th result attribute is expressed as:
$$\beta_j=\frac{\prod_{i=1}^L(m_{j,i}+m_{D,i})-\prod_{i=1}^Lm_{D,i}}{\sum_{t=1}^N\prod_{i=1}^L(m_{t,i}+m_{D,i})-N\times\prod_{i=1}^Lm_{D,i}}$$
Then the result of the j-th result attribute before normalization is expressed as:
$$\overline{\beta}_j = \prod_{i=1}^L(m_{j,i}+m_{D,i})-\prod_{i=1}^Lm_{D,i},\beta_j=\frac{\overline{\beta}_j}{\sum_{k=1}^N\overline{\beta}_k}$$
Substituting $m_{j,i}=w_i\beta_{j,i}$ and $m_{D,i}=1-w_i$ into the j-th result attribute before normalization expression:
$$\overline{\beta}_j = \prod_{i=1}^L(w_i\beta_j^i+1-w_i)-\prod_{i=1}^L{(1-w_i)}$$
We can get the partial derivative of the $i$-th result attribute $\beta_i$ to the $j$-th unnormalized result attribute $\overline{\beta}_j$ as:
$$\frac{d\beta_i}{d\overline{\beta}_j}=
    \left\{
    \begin{aligned}
        \frac{\sum_{k\neq j}^N\overline{\beta}_k}{(\sum_{k=1}^N\overline{\beta}_k)^2},j=i \\
        -\frac{\beta_i}{(\sum_{k=1}^N\overline{\beta}_k)^2},j\neq i
    \end{aligned}
    \right.$$
Similarly, the partial derivative of the $j$-th unnormalized result attribute $\overline{\beta}_j$ to the activation weight of the $k$-th rule and the $j$-th result attribute of $k$-th rule can be obtained as:
$$\frac{d\overline{\beta}_j}{dw_k}=(\beta_j^k-1)\prod_{i=1,i\neq k}^L(w_i\beta_j^i+1-w_i)+\prod_{i=1,i\neq k}^L(1-w_i)$$
$$\frac{d\overline{\beta}_j}{d\beta_j^k}=w_k\prod_{i=1,i\neq k}^L(w_i\beta_j^i+1-w_i)$$
According to the activation weight expression of the $k$-th rule, we can obtain its partial derivatives of the rule antecedent attribute parameter and the corresponding attribute distance-sensitive parameter respectively:
$$\frac{dw_k}{dx_l^k}=2(a_l^k)^2(x_l-x_l^k)e^{-\sum_{i=1}^{T_k}[a_i^k(x_i-x_i^k)]^2}$$
$$\frac{dw_k}{da_l^k}=2a_l^kx_l^k(x_l-x_l^k)e^{-\sum_{i=1}^{T_k}[a_i^k(x_i-x_i^k)]^2}$$
Set the loss function expression of the final output result to $loss=\sum_i^NLoss(\beta_i)$, according to the compound function chain derivation rule, we can obtain the partial derivative of the final loss on each parameter.
$$\frac{dloss}{d\beta_j^k}=\sum_{i=1}^N\sum_{j=1}^N\frac{dloss}{d\beta_i}\frac{d\beta_i}{d\overline{\beta}_j}\frac{d\overline{\beta}_j}{d\beta_j^k}$$
$$\frac{dloss}{dx_l^k}=\sum_{i=1}^N\sum_{j=1}^N\frac{dloss}{d\beta_i}\frac{d\beta_i}{d\overline{\beta}_j}\frac{d\overline{\beta}_j}{dw_k}\frac{dw_k}{dx_l^k}$$
$$\frac{dloss}{da_l^k}=\sum_{i=1}^N\sum_{j=1}^N\frac{dloss}{d\beta_i}\frac{d\beta_i}{d\overline{\beta}_j}\frac{d\overline{\beta}_j}{dw_k}\frac{dw_k}{da_l^k}$$

\subsection{Differentiable proof of BRB system}
The evidential reasoning process of the belief rule base is a multivariate compound function process. According to the differentiable condition of the multivariate compound function, each intermediate function must satisfy the differential condition,
and the partial derivative of each variable must exist and be continuous.
Since any of the partial derivatives above are only obtained by elementary functions through four arithmetic operations and compound,
the partial derivatives of any parameter are continuous in its domain.
The existence and continuous partial derivative of any parameter can prove that the whole inference system is differentiable.

When the appropriate loss function is selected, the final result loss is differentiable to all the parameters of the model, which provides conditions for using the gradient method to optimize the model parameters.
The gradient of the loss result on the parameters of each part of the model can be obtained. The gradient of the loss function on all the rule antecedent attribute parameters is:
$$\nabla_{x}loss=\left[\begin{matrix}
            \frac{dloss}{dx_1^1} & \cdots & \frac{dloss}{dx_{T_k}^1} \\
            \vdots               & \ddots & \vdots                   \\
            \frac{dloss}{dx_1^L} & \cdots & \frac{dloss}{dx_{T_k}^L}
        \end{matrix}\right]$$
The gradient of the loss function on the distance-sensitive parameters of all rules is:
$$\nabla_{a}loss=\left[\begin{matrix}
            \frac{dloss}{da_1^1} & \cdots & \frac{dloss}{da_{T_k}^1} \\
            \vdots               & \ddots & \vdots                   \\
            \frac{dloss}{da_1^L} & \cdots & \frac{dloss}{da_{T_k}^L}
        \end{matrix}\right]$$
The gradient of the loss function on all rule result attribute parameters is:
$$\nabla_{\beta}loss=\left[\begin{matrix}
            \frac{dloss}{d\beta_{1,1}} & \cdots & \frac{dloss}{d\beta_{N,1}} \\
            \vdots                     & \ddots & \vdots                     \\
            \frac{dloss}{d\beta_{1,L}} & \cdots & \frac{dloss}{d\beta_{N,L}} \\
        \end{matrix}\right]$$
According to the belief distribution output by the inference system and the loss function of the result,
the gradient of each part of the parameters can be optimized by updating the parameters along the negative gradient direction.

\subsection{Exponential normalization function preprocessing}
In the training process, in order to meet the restriction that the sum of the result attributes of each rule is one and each result attribute is non-negative,
we use the exponential normalization function to preprocess the result attribute parameters during the training process:
$$\beta_j^k=\frac{e^{\overline{\beta_j^k}}}{\sum_{i=1}^Ne^{\overline{\beta_i^k}}}(\sum_{i=1}^N\beta_j^k=1,\beta_j^k>0)$$

\subsection{Stochastic gradient descent with momentum optimization}
The optimization process of using the gradient descent method to update the parameters of the inference model is given by the following equation:
$$M_{new}(x,a,\beta)=M_{old}-\mu\nabla_{M_{old}}loss$$
The gradient information is given according to the loss function of the final output result, and the learning rate $\mu$ is the updated step length information that needs to be set.
Chang\cite{a11} and Wu\cite{a13} used dichotomy in the gradient training process to iteratively find the optimal step size in the constraint space and added perturbation parameters when the gradient is zero to avoid the training process stagnation.
For the application of new rule structures, activation methods, and preprocessing steps, the gradient training update step size is no longer limited.
We use momentum-optimized stochastic gradient descent method for faster training.

The stochastic gradient descent method is an iterative optimization method when the objective function is differentiable,
It used a random subset of the training data to calculate the gradient value as the estimated value of the entire training data gradient,
which reduces the computational burden in high-dimensional optimization problems.

The output of the loss function in the traditional gradient descent method is determined by all samples,
and the model parameters are updated according to its gradient:
$$loss=\frac{1}{n}\sum_i^nLoss(\beta_1^i,...,\beta_N^i)$$
We randomly selected a single sample as the estimated value of the average value of the loss function output on all samples to update the model parameters:
$$loss=Loss(\beta_1^r,...,\beta_N^r)$$

The obvious disadvantage of the stochastic gradient descent method is that its update direction is completely dependent on the gradient of the current sample and is very unstable. The momentum method is used to solve this problem.
The momentum method improves the stability and speed of the training method by retaining a certain degree of historical gradient information and combining it with the current sample gradient.
It also enhances the ability to get rid of local optimal solutions.

The momentum method uses a weighted fusion method to synthesize the historical gradient information and the current sample gradient,
and use it as an update parameter for this round of training:
$$v_t=\nu v_{t-1}+\mu\nabla_{loss}M,M=M-v_t$$
Initial $v_0$ is set to zero and set $\nu$ to represent the ratio of retaining historical gradient information.
The same direction of the current gradient and the historical gradient will increase the speed of parameter training in this gradient direction,
The different directions of the current gradient and the historical gradient will inhibit the current gradient from causing parameter training oscillations.

\section{experimental results}
In this section,
we first conduct a comparison test of the training performance of the gradient method under different momentum parameters,
and then conduct a comparison test of the performance of the BRB system and traditional machine learning algorithms on public data sets.
Finally summarize all the experimental results.

\subsection{experimental enviroment}
The experiment runs on a Ubuntu 20.04 system equipped with Intel$^\copyright$ Core$^\text{TM}$ i5 8500@3.0GHz CPU, 16GB RAM and GeForce GTX 1060 Graphics.
Use TensorFlow 2.0 to build the evidential reasoning framework of the BRB system and
use Scikit-learn machine learning library to collect and clean datasets.

\subsection{performance of gradient descent method with different momentum parameters}
Liu\cite{a16} proved that the confidence rule library can approximate any function.
In this section, a nonlinear multi-extreme function is introduced to compare the training performance of the stochastic gradient descent method under different momentum parameters.
The nonlinear multi-extreme function is as follows:
$$f(x)=e^{-(x-2)^2}+0.5e^{-(x+2)^2},x\in[-5,5]$$
In the defined domain, 1000 pieces of data are uniformly selected as the fitting data set and the mean square error is used as the loss function.
According to each extreme point on the function curve, the rule base result attribute evaluation level and corresponding utility value can be set:
$$\{D_1,D_2,D_3,D_4,D_5\}=\{-0.5,0,0.5,1,1.5\}$$
Select the five extreme points on the function curve to convert to the corresponding rule structure to build a belief rule library.
Set the number of training samples in each batch to $128$ and the learning rate $\mu$ is $0.001$ for $1000$ rounds of training.
Set the momentum optimization parameters $\nu$ to 0.0(non-momentum optimization), 0.1, 0.5 and 0.9 to compare their fitting performance.

\Figure[!t](topskip=0pt, botskip=0pt, midskip=0pt)[width=3.3 in]{process.png}
{Mean square error loss under different momentum parameters\label{fig4}}
Figure \ref{fig4} shows the mean square error loss of each batch under different momentum parameters.







\begin{thebibliography}{00}

    \bibitem{a1} YANG J B, LIU J, WANG J, et al. ``Belief rule-base inference methodology
    using the evidential reasoning approach-rimer,''
    \emph{IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
    2006, 2(36):266-285.

    \bibitem{a2} SUN R. ``Robust reasoning: integrating rule-based and similarity-based
    reasoning,'' \emph{Artificial Intelligence},
    1995, 2(75):241-295.

    \bibitem{a3} DEMPSTER A P. ``A generalization of bayesian inference,''
    \emph{Journal of the Royal Statistical Society: Series B (Methodological)},
    1968, 2(30): 205-232.

    \bibitem{a4} SHAFER G, SMITH A F M. ``A mathematical theory of evidence[J],''
    \emph{Biometrics},
    1976, 3(32):703.

    \bibitem{a5} YOON K, HWANG C L. ``Multiple attribute decision making,''
    \emph{Thousand Oaks, CA: Sage Publications},
    1995.

    \bibitem{a6} ZADEH L. ``Fuzzy sets,''
    \emph{Information and Control},
    1965, 3(8):338-353.

    \bibitem{a7} ZHOU Z J, HU C H, YANG J B, et al. ``Online updating belief rule based
    system for pipeline leak detection under expert intervention,''
    \emph{Expert Systems with Applications},
    2009, 4(36):7700-7709.

    \bibitem{a8} JIANG J, LI X, JIE ZHOU Z, et al. ``Weapon system capability assessment
    under uncertainty based on the evidential reasoning approach,''
    \emph{Expert Systems with Applications},
    2011.

    \bibitem{a9} YANG Y, FU C, CHEN Y W, et al. ``A belief rule based expert system
    for predicting consumer preference in new product development,''
    \emph{Knowledge-Based Systems},
    2016(94):105-113.

    \bibitem{a10} YANG J B, LIU J, XU D L, et al. ``Optimization models for training
    belief-rule-based systems,''
    \emph{IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
    2007, 4(37):569-585.

    \bibitem{a11} CHANG R, ZHANG S. ``An Algorithm for Training Param eters in Belief Rule-bases Based on
    Gradient M ethods with Optimization Step Size,''
    \emph{Journal of North China Institute of Water Conservancy and Hydroelectric Power},
    2011, 1(32):154-157.

    \bibitem{a12} CHANG R Y, WANG H, YANG J B. ``An algorithm for training parameters
    in belief rule-bases based on the gradient and dichotomy methods,''
    \emph{Systems Engineering},
    2007.

    \bibitem{a13} WU W K, YANG L H, FU Y G, et al. ``Parameter Training Approach for Belief Rule Base Using the Accelerating of Gradient
    Algorithm,''
    \emph{Journal of Frontiers of Computer Science and Technology},
    2014, 8(8):989-1001.

    \bibitem{a14} SU Q, YANG L H, FU Y G,et al. ``Parameter training approach based on variable particle swarm optimization
    for belief rule base,''
    \emph{Journal of Computer Applications},
    2014, 34(8):2161-2165.

    \bibitem{a15} WANG H J, YANG L H, FU Y G H, et al. ``Differential Evolutionary Algorithm for Parameter Training of Belief Rule Base under Expert Intervention,''
    \emph{Computer Science},
    2015, 42(5):88-93.

    \bibitem{a16} LIU J, MARTINEZ L, CALZADA A C, et al. ``A novel beliefrule base representation, generation and its inference methodology,''
    \emph{Knowledge-Based Systems},
    2013(53):129-141.

    \bibitem{a17} ROBBINS H, MONRO S. ``A stochastic approximation method,''
    \emph{The Annals of Mathematical Statistics},
    1951, 3(22):400-407.

    \bibitem{a18} KIWIEL K C. ``Convergence and efficiency of subgradient methods for quasiconvex minimization,''
    \emph{Mathematical Programming},
    2001,1(90):1-25.

    \bibitem{a19} RUMELHART D E, HINTON G E, WILLIAMS R J. ``Learning representations
    by back-propagating errors,''
    \emph{Nature},
    1986, 6088(323):533-536.

    \bibitem{a20} LIN Y Q, FU Y G. ``A rule activation method for extended belief rule base based on improved similarity measures,''
    \emph{Journal of University of Science and Technology of China},
    2018, 48(1):21-27.

    \bibitem{a21} LIN Y Q, FU Y G, SU Q, et al. ``A rule activation method for extended
    belief rule base with vp-tree and mvp-tree,''
    \emph{Journal of Intelligent and Fuzzy Systems},
    2017, 33(6):3695-3705.

    \bibitem{a22} FANG W, GONG X, LIU G, et al. ``A balance adjusting approach of extended
    belief-rule-based system for imbalanced classification problem,''
    \emph{IEEE Access},
    2020, 8(19419049):41201-41212.

    \bibitem{a23} SHANNON C E. ``A mathematical theory of communication,''
    \emph{Bell System Technical Journal},
    1948, 4(27):623-656.

    \bibitem{a24} CALZADA A, LIU J, WANG H, et al. ``A new dynamic rule activation
    method for extended belief rule-based systems,''
    \emph{IEEE Transactions on Knowledge and Data Engineering},
    2015, 4(27):880-894.

    \bibitem{a25} YANG L H. ``New activation weight calculation and parameter optimization
    for extended belief rule-based system based on sensitivity analysis,''
    \emph{Knowledge and Information Systems},
    2018, 60(2):837-878.

    \bibitem{a26} WANG Y. ``Parameter learning for an intuitionistic fuzzy belief rulebased
    systems based on weight and reliability,''
    \emph{Journal of Advanced Computational Intelligence and Intelligent Informatics},
    2019, 23(2):219-228.

    \bibitem{a27} ZHU H. ``A minimum centre distance rule activation method for extended
    belief rule-based classification systems,''
    \emph{Applied Soft Computing},
    2020, 91:106214.

    \bibitem{a28} JIA Q. ``A novel fault detection model based on atanassovâ€™s intervalvalued
    intuitionistic fuzzy sets, belief rule base and evidential reasoning,''
    \emph{IEEE Access}, 2020, 8:4551-4567.


\end{thebibliography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a1.png}}]{First A. Author} (M'76--SM'81--F'87) and all authors may include
    biographies. Biographies are often not included in conference-related
    papers. This author became a Member (M) of IEEE in 1976, a Senior
    Member (SM) in 1981, and a Fellow (F) in 1987. The first paragraph may
    contain a place and/or date of birth (list place, then date). Next,
    the author's educational background is listed. The degrees should be
    listed with type of degree in what field, which institution, city,
    state, and country, and year the degree was earned. The author's major
    field of study should be lower-cased.

    The second paragraph uses the pronoun of the person (he or she) and not the
    author's last name. It lists military and work experience, including summer
    and fellowship jobs. Job titles are capitalized. The current job must have a
    location; previous positions may be listed
    without one. Information concerning previous publications may be included.
    Try not to list more than three books or published articles. The format for
    listing publishers of a book within the biography is: title of book
    (publisher name, year) similar to a reference. Current and previous research
    interests end the paragraph. The third paragraph begins with the author's
    title and last name (e.g., Dr.\ Smith, Prof.\ Jones, Mr.\ Kajor, Ms.\ Hunter).
    List any memberships in professional societies other than the IEEE. Finally,
    list any awards and work for IEEE committees and publications. If a
    photograph is provided, it should be of good quality, and
    professional-looking. Following are two examples of an author's biography.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a2.png}}]{Second B. Author} was born in Greenwich Village, New York, NY, USA in
    1977. He received the B.S. and M.S. degrees in aerospace engineering from
    the University of Virginia, Charlottesville, in 2001 and the Ph.D. degree in
    mechanical engineering from Drexel University, Philadelphia, PA, in 2008.

    From 2001 to 2004, he was a Research Assistant with the Princeton Plasma
    Physics Laboratory. Since 2009, he has been an Assistant Professor with the
    Mechanical Engineering Department, Texas A{\&}M University, College Station.
    He is the author of three books, more than 150 articles, and more than 70
    inventions. His research interests include high-pressure and high-density
    nonthermal plasma discharge processes and applications, microscale plasma
    discharges, discharges in liquids, spectroscopic diagnostics, plasma
    propulsion, and innovation plasma applications. He is an Associate Editor of
    the journal \emph{Earth, Moon, Planets}, and holds two patents.

    Dr. Author was a recipient of the International Association of Geomagnetism
    and Aeronomy Young Scientist Award for Excellence in 2008, and the IEEE
    Electromagnetic Compatibility Society Best Symposium Paper Award in 2011.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a3.png}}]{Third C. Author, Jr.} (M'87) received the B.S. degree in mechanical
    engineering from National Chung Cheng University, Chiayi, Taiwan, in 2004
    and the M.S. degree in mechanical engineering from National Tsing Hua
    University, Hsinchu, Taiwan, in 2006. He is currently pursuing the Ph.D.
    degree in mechanical engineering at Texas A{\&}M University, College
    Station, TX, USA.

    From 2008 to 2009, he was a Research Assistant with the Institute of
    Physics, Academia Sinica, Tapei, Taiwan. His research interest includes the
    development of surface processing and biological/medical treatment
    techniques using nonthermal atmospheric pressure plasmas, fundamental study
    of plasma sources, and fabrication of micro- or nanostructured surfaces.

    Mr. Author's awards and honors include the Frew Fellowship (Australian
    Academy of Science), the I. I. Rabi Prize (APS), the European Frequency and
    Time Forum Award, the Carl Zeiss Research Award, the William F. Meggers
    Award and the Adolph Lomb Medal (OSA).
\end{IEEEbiography}

\EOD

\end{document}
